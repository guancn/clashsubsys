"""
è§„åˆ™å¤„ç†å™¨ - å¤„ç†è¿œç¨‹é…ç½®å’Œè‡ªå®šä¹‰è§„åˆ™
"""

import re
import httpx
import logging
from typing import List, Dict, Any, Optional, Tuple
from configparser import ConfigParser
from io import StringIO

from ..models.schemas import ProxyGroup, Rule

logger = logging.getLogger(__name__)


class RuleProcessor:
    """è§„åˆ™å¤„ç†å™¨"""
    
    def __init__(self):
        self.default_groups = [
            {
                "name": "ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
                "type": "select",
                "proxies": ["â™»ï¸ è‡ªåŠ¨é€‰æ‹©", "ğŸ”— æ•…éšœè½¬ç§»", "ğŸ”„ è´Ÿè½½å‡è¡¡", "DIRECT"]
            },
            {
                "name": "â™»ï¸ è‡ªåŠ¨é€‰æ‹©",
                "type": "url-test",
                "url": "http://www.gstatic.com/generate_204",
                "interval": 300,
                "tolerance": 50,
                "proxies": []
            },
            {
                "name": "ğŸ”— æ•…éšœè½¬ç§»",
                "type": "fallback",
                "url": "http://www.gstatic.com/generate_204",
                "interval": 300,
                "proxies": []
            },
            {
                "name": "ğŸ”„ è´Ÿè½½å‡è¡¡",
                "type": "load-balance",
                "strategy": "consistent-hashing",
                "url": "http://www.gstatic.com/generate_204",
                "interval": 300,
                "proxies": []
            },
            {
                "name": "ğŸŸ æ¼ç½‘ä¹‹é±¼",
                "type": "select",
                "proxies": ["ğŸš€ èŠ‚ç‚¹é€‰æ‹©", "DIRECT", "REJECT"]
            },
            {
                "name": "ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
                "type": "select",
                "proxies": ["ğŸš€ èŠ‚ç‚¹é€‰æ‹©", "â™»ï¸ è‡ªåŠ¨é€‰æ‹©", "DIRECT"]
            },
            {
                "name": "ğŸ¯ å…¨çƒç›´è¿",
                "type": "select",
                "proxies": ["DIRECT", "ğŸš€ èŠ‚ç‚¹é€‰æ‹©"]
            },
            {
                "name": "ğŸ›‘ å¹¿å‘Šæ‹¦æˆª",
                "type": "select",
                "proxies": ["REJECT", "DIRECT"]
            },
            {
                "name": "ğŸƒ åº”ç”¨å‡€åŒ–",
                "type": "select",
                "proxies": ["REJECT", "DIRECT"]
            }
        ]
        
        self.default_rules = [
            # ç®¡ç†ç•Œé¢ç›´è¿
            "DOMAIN,clash.razord.top,DIRECT",
            "DOMAIN,yacd.haishan.me,DIRECT",
            
            # å±€åŸŸç½‘ç›´è¿
            "IP-CIDR,192.168.0.0/16,DIRECT",
            "IP-CIDR,10.0.0.0/8,DIRECT", 
            "IP-CIDR,172.16.0.0/12,DIRECT",
            "IP-CIDR,127.0.0.0/8,DIRECT",
            "IP-CIDR,100.64.0.0/10,DIRECT",
            "IP-CIDR,224.0.0.0/4,DIRECT",
            "IP-CIDR6,fe80::/10,DIRECT",
            
            # Apple æœåŠ¡ç›´è¿
            "DOMAIN-SUFFIX,apple.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,icloud.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,apple-cloudkit.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,crashlytics.com,ğŸ¯ å…¨çƒç›´è¿",
            
            # å¸¸è§ç›´è¿æœåŠ¡
            "DOMAIN-SUFFIX,qq.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,taobao.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,tmall.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,alipay.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,baidu.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,163.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,126.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,weibo.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,sina.com.cn,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,douban.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,zhihu.com,ğŸ¯ å…¨çƒç›´è¿",
            "DOMAIN-SUFFIX,bilibili.com,ğŸ¯ å…¨çƒç›´è¿",
            
            # Telegram
            "DOMAIN-SUFFIX,t.me,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "DOMAIN-SUFFIX,tdesktop.com,ğŸ“² ç”µæŠ¥æ¶ˆæ¯", 
            "DOMAIN-SUFFIX,telegra.ph,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "DOMAIN-SUFFIX,telegram.org,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "IP-CIDR,91.108.4.0/22,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "IP-CIDR,91.108.8.0/22,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "IP-CIDR,91.108.12.0/22,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "IP-CIDR,91.108.16.0/22,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "IP-CIDR,91.108.56.0/22,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            "IP-CIDR,149.154.160.0/20,ğŸ“² ç”µæŠ¥æ¶ˆæ¯",
            
            # å¸¸è§ä»£ç†æœåŠ¡
            "DOMAIN-SUFFIX,google.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,youtube.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,facebook.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,twitter.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,instagram.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,github.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,netflix.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,openai.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            "DOMAIN-SUFFIX,chatgpt.com,ğŸš€ èŠ‚ç‚¹é€‰æ‹©",
            
            # åœ°ç†ä½ç½®è§„åˆ™
            "GEOIP,LAN,DIRECT",
            "GEOIP,CN,ğŸ¯ å…¨çƒç›´è¿",
            
            # æœ€ç»ˆåŒ¹é…
            "MATCH,ğŸŸ æ¼ç½‘ä¹‹é±¼"
        ]
    
    async def fetch_remote_config(self, url: str) -> Optional[Dict[str, Any]]:
        """
        è·å–è¿œç¨‹é…ç½®æ–‡ä»¶
        
        Args:
            url: è¿œç¨‹é…ç½®æ–‡ä»¶ URL
            
        Returns:
            è§£æåçš„é…ç½®å­—å…¸
        """
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url)
                response.raise_for_status()
                
                content = response.text
                
                # å°è¯•è§£æ INI æ ¼å¼
                if self._is_ini_format(content):
                    return self._parse_ini_config(content)
                
                # å°è¯•è§£æå…¶ä»–æ ¼å¼ï¼ˆå¦‚ YAMLï¼‰
                # è¿™é‡Œå¯ä»¥æ‰©å±•æ”¯æŒæ›´å¤šæ ¼å¼
                logger.warning(f"Unsupported config format from {url}")
                return None
                
        except Exception as e:
            logger.error(f"Failed to fetch remote config from {url}: {e}")
            return None
    
    def _is_ini_format(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸º INI æ ¼å¼æˆ–è‡ªå®šä¹‰æ ¼å¼"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡ªå®šä¹‰ä»£ç†ç»„æˆ–è§„åˆ™é›†é…ç½®
        return ('custom_proxy_group' in content or 
                'ruleset' in content or 
                'enable_rule_generator' in content or
                ('[' in content and ']' in content and '=' in content))
    
    def _parse_ini_config(self, content: str) -> Dict[str, Any]:
        """
        è§£æé…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒINIæ ¼å¼å’Œè‡ªå®šä¹‰æ ¼å¼ï¼‰
        
        Args:
            content: é…ç½®å†…å®¹
            
        Returns:
            è§£æåçš„é…ç½®å­—å…¸
        """
        try:
            result = {
                'custom_proxy_group': [],
                'ruleset': [],
                'rename': [],
                'template': {},
            }
            
            # é¦–å…ˆå°è¯•æŒ‰è¡Œè§£æè‡ªå®šä¹‰æ ¼å¼
            lines = content.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                
                # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                if not line or line.startswith('#') or line.startswith(';'):
                    continue
                
                # è§£æ ruleset è¡Œ
                if line.startswith('ruleset='):
                    ruleset_config = line[8:]  # ç§»é™¤ "ruleset=" å‰ç¼€
                    result['ruleset'].append(ruleset_config)
                
                # è§£æ custom_proxy_group è¡Œ
                elif line.startswith('custom_proxy_group='):
                    group_config = line[19:]  # ç§»é™¤ "custom_proxy_group=" å‰ç¼€
                    result['custom_proxy_group'].append(group_config)
                
                # è§£æå…¶ä»–é…ç½®é¡¹ï¼ˆä½œä¸ºæ¨¡æ¿é…ç½®ï¼‰
                elif '=' in line and not line.startswith('['):
                    key, value = line.split('=', 1)
                    result['template'][key.strip()] = value.strip()
            
            # å¦‚æœè‡ªå®šä¹‰æ ¼å¼è§£æå¤±è´¥ï¼Œå°è¯•æ ‡å‡†INIæ ¼å¼
            if not result['ruleset'] and not result['custom_proxy_group']:
                try:
                    config = ConfigParser()
                    config.read_string(content)
                    
                    # è§£æ custom_proxy_group éƒ¨åˆ†
                    if config.has_section('custom_proxy_group'):
                        for key, value in config.items('custom_proxy_group'):
                            result['custom_proxy_group'].append(value)
                    
                    # è§£æ ruleset éƒ¨åˆ†
                    if config.has_section('ruleset'):
                        for key, value in config.items('ruleset'):
                            result['ruleset'].append(value)
                    
                    # è§£æ template éƒ¨åˆ†
                    if config.has_section('template'):
                        for key, value in config.items('template'):
                            result['template'][key] = value
                    
                    # è§£æèŠ‚ç‚¹é‡å‘½åè§„åˆ™
                    if config.has_section('rename_node'):
                        for key, value in config.items('rename_node'):
                            result['rename'].append(f"{key},{value}")
                
                except Exception as ini_error:
                    logger.warning(f"Standard INI parsing also failed: {ini_error}")
            
            logger.info(f"Parsed remote config: {len(result['ruleset'])} rulesets, {len(result['custom_proxy_group'])} proxy groups")
            return result
            
        except Exception as e:
            logger.error(f"Failed to parse config: {e}")
            return {}
    
    def generate_proxy_groups(self, 
                            nodes: List[str], 
                            custom_groups: Optional[List[str]] = None,
                            node_filters: Optional[Dict[str, str]] = None) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆä»£ç†ç»„é…ç½®
        
        Args:
            nodes: èŠ‚ç‚¹åç§°åˆ—è¡¨
            custom_groups: è‡ªå®šä¹‰ä»£ç†ç»„é…ç½®
            node_filters: èŠ‚ç‚¹è¿‡æ»¤è§„åˆ™
            
        Returns:
            ä»£ç†ç»„é…ç½®åˆ—è¡¨
        """
        try:
            groups = []
            
            # å¤„ç†è‡ªå®šä¹‰ä»£ç†ç»„
            if custom_groups:
                for group_config in custom_groups:
                    group = self._parse_custom_group(group_config, nodes, node_filters)
                    if group:
                        groups.append(group)
            else:
                # ä½¿ç”¨é»˜è®¤ä»£ç†ç»„
                for default_group in self.default_groups:
                    group = dict(default_group)
                    
                    # ä¸ºéœ€è¦èŠ‚ç‚¹çš„ç»„æ·»åŠ èŠ‚ç‚¹
                    if group['name'] in ['â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸ”— æ•…éšœè½¬ç§»', 'ğŸ”„ è´Ÿè½½å‡è¡¡']:
                        group['proxies'] = nodes[:]  # å¤åˆ¶èŠ‚ç‚¹åˆ—è¡¨
                    elif group['name'] == 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©':
                        # èŠ‚ç‚¹é€‰æ‹©ç»„åŒ…å«å…¶ä»–ç­–ç•¥ç»„å’Œæ‰€æœ‰èŠ‚ç‚¹
                        strategy_groups = ["â™»ï¸ è‡ªåŠ¨é€‰æ‹©", "ğŸ”— æ•…éšœè½¬ç§»", "ğŸ”„ è´Ÿè½½å‡è¡¡", "DIRECT"]
                        group['proxies'] = strategy_groups + nodes
                    
                    groups.append(group)
            
            return groups
            
        except Exception as e:
            logger.error(f"Failed to generate proxy groups: {e}")
            return self.default_groups
    
    def _parse_custom_group(self, 
                          group_config: str, 
                          nodes: List[str],
                          node_filters: Optional[Dict[str, str]] = None) -> Optional[Dict[str, Any]]:
        """
        è§£æè‡ªå®šä¹‰ä»£ç†ç»„é…ç½®
        
        æ ¼å¼: Group_Name`select`[]Group_1[]Group_2[].*HK.*`http://www.gstatic.com/generate_204`300
        """
        try:
            parts = group_config.split('`')
            if len(parts) < 3:
                return None
            
            name = parts[0]
            group_type = parts[1]
            
            # è§£æä»£ç†åˆ—è¡¨éƒ¨åˆ†
            proxies_part = parts[2]
            url = parts[3] if len(parts) > 3 else "http://www.gstatic.com/generate_204"
            
            # è§£æç¬¬4éƒ¨åˆ†ï¼Œæ ¼å¼å¯èƒ½æ˜¯ "300,,50" 
            if len(parts) > 4:
                interval_tolerance_part = parts[4]
                # æŒ‰é€—å·åˆ†å‰²ï¼Œå¯èƒ½æœ‰å¤šä¸ªé€—å·
                interval_parts = interval_tolerance_part.split(',')
                
                # ç¬¬ä¸€ä¸ªæ˜¯interval
                interval = int(interval_parts[0]) if interval_parts[0].isdigit() else 300
                
                # æœ€åä¸€ä¸ªéç©ºéƒ¨åˆ†æ˜¯tolerance
                tolerance = 50
                for part in reversed(interval_parts):
                    if part.strip() and part.strip().isdigit():
                        tolerance = int(part.strip())
                        break
            else:
                interval = 300
                tolerance = 50
            
            # è§£æä»£ç†
            proxies = []
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåœ°åŒºåŒ¹é…æ ¼å¼ï¼ˆä»¥ç‚¹å¼€å¤´å’Œç»“å°¾ï¼‰
            if proxies_part.startswith('.') and proxies_part.endswith('.'):
                # åœ°åŒºåŒ¹é…æ ¼å¼ï¼Œå¦‚ .é¦™æ¸¯|HK.
                pattern_str = proxies_part[1:-1]  # ç§»é™¤é¦–å°¾çš„ç‚¹
                try:
                    pattern = re.compile(pattern_str, re.IGNORECASE)
                    filtered_nodes = [node for node in nodes if pattern.search(node)]
                    proxies.extend(filtered_nodes)
                    logger.info(f"Regional group {name} matched {len(filtered_nodes)} nodes with pattern: {pattern_str}")
                except re.error as e:
                    logger.warning(f"Invalid regex pattern {proxies_part}: {e}")
                    # å¦‚æœæ­£åˆ™å¤±è´¥ï¼Œå°è¯•ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
                    keywords = pattern_str.split('|')
                    for node in nodes:
                        if any(keyword in node for keyword in keywords):
                            proxies.append(node)
            else:
                # æŒ‰ [] åˆ†å‰²çš„ä¼ ç»Ÿæ ¼å¼
                proxy_items = re.findall(r'\[(.*?)\]', proxies_part)
                
                for item in proxy_items:
                    if item == 'DIRECT' or item == 'REJECT':
                        proxies.append(item)
                    elif item.startswith('[]'):
                        # å¼•ç”¨å…¶ä»–ç»„
                        proxies.append(item[2:])
                    else:
                        # èŠ‚ç‚¹è¿‡æ»¤è§„åˆ™
                        if '.*' in item or item.startswith('^') or item.endswith('$'):
                            # æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤
                            try:
                                pattern = re.compile(item, re.IGNORECASE)
                                filtered_nodes = [node for node in nodes if pattern.search(node)]
                                proxies.extend(filtered_nodes)
                            except re.error as e:
                                logger.warning(f"Invalid regex pattern {item}: {e}")
                        else:
                            # ç›´æ¥æ·»åŠ 
                            if item in nodes:
                                proxies.append(item)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°èŠ‚ç‚¹ï¼Œæ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
            if not proxies and group_type in ['url-test', 'fallback', 'load-balance']:
                proxies = nodes[:]
            
            group = {
                'name': name,
                'type': group_type,
                'proxies': proxies
            }
            
            # æ·»åŠ æµ‹è¯•ç›¸å…³é…ç½®
            if group_type in ['url-test', 'fallback', 'load-balance']:
                group['url'] = url
                group['interval'] = interval
                
                if group_type == 'url-test':
                    group['tolerance'] = tolerance
                elif group_type == 'load-balance':
                    group['strategy'] = 'consistent-hashing'
            
            return group
            
        except Exception as e:
            logger.error(f"Failed to parse custom group: {group_config}, error: {e}")
            return None
    
    def generate_rules(self, 
                      custom_rulesets: Optional[List[str]] = None,
                      custom_rules: Optional[List[str]] = None) -> tuple[List[str], Dict[str, Any]]:
        """
        ç”Ÿæˆè§„åˆ™åˆ—è¡¨å’Œrule-providersé…ç½®
        
        Args:
            custom_rulesets: è‡ªå®šä¹‰è§„åˆ™é›†é…ç½®
            custom_rules: è‡ªå®šä¹‰è§„åˆ™åˆ—è¡¨
            
        Returns:
            (è§„åˆ™åˆ—è¡¨, rule-providersé…ç½®å­—å…¸)
        """
        try:
            rules = []
            rule_providers = {}
            
            # å¤„ç†è‡ªå®šä¹‰è§„åˆ™é›†
            if custom_rulesets:
                for ruleset_config in custom_rulesets:
                    parsed_rule = self._parse_custom_ruleset(ruleset_config)
                    if parsed_rule:
                        if parsed_rule.get('rule') == 'MATCH':
                            # å¤„ç†FINALè§„åˆ™
                            rules.append(f"MATCH,{parsed_rule['policy']}")
                        elif parsed_rule.get('url'):
                            # å¤„ç†RULE-SETè§„åˆ™
                            rule_name = parsed_rule['name']
                            rule_url = parsed_rule['url']
                            policy = parsed_rule['policy']
                            
                            # æ·»åŠ åˆ°rule-providers
                            rule_providers[rule_name] = {
                                "type": "http",
                                "behavior": "domain" if "domain" in rule_name.lower() else "ipcidr" if "ip" in rule_name.lower() else "classical",
                                "url": rule_url,
                                "path": f"./{rule_name}.yaml",
                                "interval": 86400
                            }
                            
                            # æ·»åŠ è§„åˆ™
                            rules.append(f"RULE-SET,{rule_name},{policy}")
            
            # æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
            if custom_rules:
                rules.extend(custom_rules)
            
            # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰è§„åˆ™ï¼Œä½¿ç”¨é»˜è®¤è§„åˆ™
            if not rules:
                rules = self.default_rules[:]
            
            logger.info(f"Generated {len(rules)} rules and {len(rule_providers)} rule providers")
            return rules, rule_providers
            
        except Exception as e:
            logger.error(f"Failed to generate rules: {e}")
            return self.default_rules, {}
    
    def _parse_custom_ruleset(self, ruleset_config: str) -> Optional[Dict[str, str]]:
        """
        è§£æè‡ªå®šä¹‰è§„åˆ™é›†é…ç½®
        
        æ ¼å¼: ç­–ç•¥ç»„,https://raw.githubusercontent.com/...,è§„åˆ™åç§°
        è¿”å›: {"policy": "ç­–ç•¥ç»„", "url": "è§„åˆ™URL", "name": "è§„åˆ™åç§°"}
        """
        try:
            # è§£ææ ¼å¼ï¼šç­–ç•¥ç»„,URL
            parts = ruleset_config.split(',', 1)
            if len(parts) == 2:
                policy = parts[0].strip()
                url = parts[1].strip()
                
                # å¤„ç†ç‰¹æ®Šè§„åˆ™
                if url == "[]FINAL":
                    return {"policy": policy, "rule": "MATCH", "name": "FINAL"}
                
                # ä»URLç”Ÿæˆè§„åˆ™åç§°
                if url.startswith('http'):
                    rule_name = url.split('/')[-1].replace('.list', '').replace('.txt', '').replace('.yaml', '')
                    return {
                        "policy": policy,
                        "url": url,
                        "name": rule_name,
                        "rule": f"RULE-SET,{rule_name}"
                    }
                
            return None
            
        except Exception as e:
            logger.error(f"Failed to parse custom ruleset: {ruleset_config}, error: {e}")
            return None
    
    def apply_node_filters(self, 
                          nodes: List[str],
                          include_pattern: Optional[str] = None,
                          exclude_pattern: Optional[str] = None) -> List[str]:
        """
        åº”ç”¨èŠ‚ç‚¹è¿‡æ»¤è§„åˆ™
        
        Args:
            nodes: åŸå§‹èŠ‚ç‚¹åˆ—è¡¨
            include_pattern: åŒ…å«è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            exclude_pattern: æ’é™¤è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            filtered_nodes = nodes[:]
            
            # åº”ç”¨åŒ…å«è§„åˆ™
            if include_pattern:
                pattern = re.compile(include_pattern, re.IGNORECASE)
                filtered_nodes = [node for node in filtered_nodes if pattern.search(node)]
            
            # åº”ç”¨æ’é™¤è§„åˆ™
            if exclude_pattern:
                pattern = re.compile(exclude_pattern, re.IGNORECASE)
                filtered_nodes = [node for node in filtered_nodes if not pattern.search(node)]
            
            return filtered_nodes
            
        except Exception as e:
            logger.error(f"Failed to apply node filters: {e}")
            return nodes
    
    def apply_node_rename(self, 
                         nodes: List[str], 
                         rename_rules: Optional[List[str]] = None) -> Dict[str, str]:
        """
        åº”ç”¨èŠ‚ç‚¹é‡å‘½åè§„åˆ™
        
        Args:
            nodes: èŠ‚ç‚¹åç§°åˆ—è¡¨
            rename_rules: é‡å‘½åè§„åˆ™åˆ—è¡¨ï¼Œæ ¼å¼: "old_pattern,new_pattern"
            
        Returns:
            é‡å‘½åæ˜ å°„å­—å…¸ {old_name: new_name}
        """
        try:
            rename_map = {}
            
            if not rename_rules:
                return {node: node for node in nodes}
            
            for node in nodes:
                new_name = node
                
                for rule in rename_rules:
                    try:
                        parts = rule.split(',', 1)
                        if len(parts) == 2:
                            pattern, replacement = parts
                            new_name = re.sub(pattern, replacement, new_name, flags=re.IGNORECASE)
                    except Exception as e:
                        logger.warning(f"Failed to apply rename rule {rule}: {e}")
                        continue
                
                rename_map[node] = new_name
            
            return rename_map
            
        except Exception as e:
            logger.error(f"Failed to apply node rename: {e}")
            return {node: node for node in nodes}
    
    def add_emoji_flags(self, node_name: str) -> str:
        """
        ä¸ºèŠ‚ç‚¹åç§°æ·»åŠ å›½æ—— Emoji
        
        Args:
            node_name: åŸå§‹èŠ‚ç‚¹åç§°
            
        Returns:
            æ·»åŠ  Emoji åçš„èŠ‚ç‚¹åç§°
        """
        flag_map = {
            # ä¸­å›½åœ°åŒº
            r'æ¸¯|hk|hong.?kong': 'ğŸ‡­ğŸ‡°',
            r'å°|tw|taiwan': 'ğŸ‡¹ğŸ‡¼',
            r'æ¾³é—¨|macao': 'ğŸ‡²ğŸ‡´',
            r'ä¸­å›½|china|cn': 'ğŸ‡¨ğŸ‡³',
            
            # äºšæ´²
            r'æ—¥æœ¬|jp|japan': 'ğŸ‡¯ğŸ‡µ',
            r'éŸ©å›½|kr|korea': 'ğŸ‡°ğŸ‡·',
            r'æ–°åŠ å¡|sg|singapore': 'ğŸ‡¸ğŸ‡¬',
            r'é©¬æ¥è¥¿äºš|my|malaysia': 'ğŸ‡²ğŸ‡¾',
            r'æ³°å›½|th|thailand': 'ğŸ‡¹ğŸ‡­',
            r'å°åº¦|in|india': 'ğŸ‡®ğŸ‡³',
            r'è²å¾‹å®¾|ph|philippines': 'ğŸ‡µğŸ‡­',
            r'å°å°¼|id|indonesia': 'ğŸ‡®ğŸ‡©',
            r'è¶Šå—|vn|vietnam': 'ğŸ‡»ğŸ‡³',
            
            # æ¬§æ´²
            r'è‹±å›½|uk|britain|united.?kingdom': 'ğŸ‡¬ğŸ‡§',
            r'æ³•å›½|fr|france': 'ğŸ‡«ğŸ‡·',
            r'å¾·å›½|de|germany': 'ğŸ‡©ğŸ‡ª',
            r'è·å…°|nl|netherlands': 'ğŸ‡³ğŸ‡±',
            r'æ„å¤§åˆ©|it|italy': 'ğŸ‡®ğŸ‡¹',
            r'è¥¿ç­ç‰™|es|spain': 'ğŸ‡ªğŸ‡¸',
            r'ä¿„ç½—æ–¯|ru|russia': 'ğŸ‡·ğŸ‡º',
            r'ç‘å£«|ch|switzerland': 'ğŸ‡¨ğŸ‡­',
            r'ç‘å…¸|se|sweden': 'ğŸ‡¸ğŸ‡ª',
            r'æŒªå¨|no|norway': 'ğŸ‡³ğŸ‡´',
            r'èŠ¬å…°|fi|finland': 'ğŸ‡«ğŸ‡®',
            r'ä¸¹éº¦|dk|denmark': 'ğŸ‡©ğŸ‡°',
            r'æ³¢å…°|pl|poland': 'ğŸ‡µğŸ‡±',
            r'åœŸè€³å…¶|tr|turkey': 'ğŸ‡¹ğŸ‡·',
            
            # ç¾æ´²
            r'ç¾å›½|us|united.?states|america': 'ğŸ‡ºğŸ‡¸',
            r'åŠ æ‹¿å¤§|ca|canada': 'ğŸ‡¨ğŸ‡¦',
            r'å¢¨è¥¿å“¥|mx|mexico': 'ğŸ‡²ğŸ‡½',
            r'å·´è¥¿|br|brazil': 'ğŸ‡§ğŸ‡·',
            r'é˜¿æ ¹å»·|ar|argentina': 'ğŸ‡¦ğŸ‡·',
            
            # å¤§æ´‹æ´²
            r'æ¾³å¤§åˆ©äºš|au|australia': 'ğŸ‡¦ğŸ‡º',
            r'æ–°è¥¿å…°|nz|new.?zealand': 'ğŸ‡³ğŸ‡¿',
            
            # éæ´²
            r'å—é|za|south.?africa': 'ğŸ‡¿ğŸ‡¦',
            r'åŸƒåŠ|eg|egypt': 'ğŸ‡ªğŸ‡¬',
            
            # ä¸­ä¸œ
            r'ä»¥è‰²åˆ—|il|israel': 'ğŸ‡®ğŸ‡±',
            r'é˜¿è”é…‹|ae|uae': 'ğŸ‡¦ğŸ‡ª',
        }
        
        for pattern, flag in flag_map.items():
            if re.search(pattern, node_name, re.IGNORECASE):
                # å¦‚æœèŠ‚ç‚¹åç§°ä¸­è¿˜æ²¡æœ‰è¿™ä¸ªå›½æ——ï¼Œåˆ™æ·»åŠ 
                if flag not in node_name:
                    return f"{flag} {node_name}"
                return node_name
        
        return node_name